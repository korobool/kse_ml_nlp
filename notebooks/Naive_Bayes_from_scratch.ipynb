{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KmIzrWXYxh5f"
   },
   "source": [
    "# Naïve Bayes from Scratch !\n",
    "\n",
    "## A Quick Short Intro to The Naïve Bayes Algorithm\n",
    "\n",
    "Naive Bayes is one of the most common ML algorithms that is often used for the purpose of text classification. If you have just stepped into ML, it is one of the easiest classification algorithms to start with. Naive Bayes is a probabilistic classification algorithm as it uses probability to make predictions for the purpose of classification.\n",
    "\n",
    "## Training Phase of The Naïve Bayes Model\n",
    "\n",
    "Let’s say, there is a restaurant review, “Very good food and service!!!”, and you want to predict that whether this given review implies a positive or a negative sentiment. To do this, we will first need to train a model ( that essentially means to determine counts of words of each category) on a relevant labelled training data set and then this model itself will be able to automatically classify such reviews into one of the given sentiments against which it was trained for. Assume that you are given a training dataset which looks like something below (a review and it’s corresponding sentiment):\n",
    "\n",
    "</br>\n",
    "\n",
    "\n",
    "<table>\n",
    "  <tr><td><b>Training Examples</b></td><td><b>Labels</b></td></tr>\n",
    "  <tr><td>Simply loved it!</td><td>Positive</td></tr>\n",
    "  <tr><td>Most disgusting food I have ever had</td><td>Negative</td></tr>\n",
    "  <tr><td>Stay away, very disgusting food</td><td>Negative</td></tr>\n",
    "  <tr><td>Menu is absolutely perfect, loved it!</td><td>Positive</td></tr>\n",
    "  <tr><td>A really good value for money</td><td>Positive</td></tr>\n",
    "  <tr><td>This is a very good restaurant</td><td>Positive</td></tr>\n",
    "  <tr><td>Terrible experience!</td><td>Negative</td></tr>\n",
    "  <tr><td>This place has best food</td><td>Positive</td></tr>\n",
    "  <tr><td>This place has most pathetic serving food!</td><td>Negative</td></tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "</br>\n",
    "\n",
    "\n",
    "Naive Bayes Classifier is a Supervised Machine Learning Algorithm\n",
    "\n",
    "### Step # 1 : Data Preprocessing\n",
    "\n",
    "As part of the preprocessing phase, all words in the training corpus/ training dataset are converted to lowercase and everything apart from letters like punctuation is excluded from the training examples.\n",
    "\n",
    "</br>\n",
    "\n",
    "<b><i>A Quick Side Note :</i></b>  A common pitfall is not preprocessing the test data in the same way as the training dataset was preprocessed and rather feeding the test example directly into the trained model. As a result, the trained model performs badly on the given test example on which it was supposed to perform quite good!\n",
    "\n",
    "</br>\n",
    "\n",
    "Preprocessed Training Dataset:\n",
    "\n",
    "</br>\n",
    "\n",
    "<table>\n",
    "  <tr><td><b>Training Examples</b></td><td><b>Labels</b></td></tr>\n",
    "  <tr><td>simply loved it</td><td>Positive</td></tr>\n",
    "  <tr><td>most disgusting food i have ever had</td><td>Negative</td></tr>\n",
    "  <tr><td>stay away very disgusting food</td><td>Negative</td></tr>\n",
    "  <tr><td>menu is absolutely perfect loved it</td><td>Positive</td></tr>\n",
    "  <tr><td>a really good value for money</td><td>Positive</td></tr>\n",
    "  <tr><td>this is a very good restaurant</td><td>Positive</td></tr>\n",
    "  <tr><td>terrible experience</td><td>Negative</td></tr>\n",
    "  <tr><td>this place has best food</td><td>Positive</td></tr>\n",
    "  <tr><td>this place has most pathetic serving food</td><td>Negative</td></tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "</br>\n",
    "\n",
    "\n",
    "### Step #2 : Training Your Naïve Bayes Model\n",
    "\n",
    "Just simply make two bag of words (BoW), one for each category, and each of them will simply contain words and their corresponding counts. All words belonging to “Positive” sentiment/label will go to one BoW and all words belonging to “Negative” sentiment will have their own BoW. Every sentence in training set is split into words (on the basis of space as a tokenizer/separator) and this is how simply word-count pairs are constructed as demonstrated below :\n",
    "\n",
    "\n",
    "</br>\n",
    "\n",
    "<table align=left>\n",
    "  <th>Positive BoW</th>\n",
    "  <tr><td><b>Words</b></td><td><b>Counts</b></td></tr>\n",
    "  <tr><td>this</td><td>2</td></tr>\n",
    "  <tr><td>loved</td><td>2</td></tr>\n",
    "  <tr><td>it</td><td>2</td></tr>\n",
    "  <tr><td>is</td><td>2</td></tr>\n",
    "  <tr><td>good</td><td>2</td></tr>\n",
    "  <tr><td>a</td><td>2</td></tr>\n",
    "  <tr><td>very</td><td>1</td></tr>\n",
    "  <tr><td>value</td><td>1</td></tr>\n",
    "  <tr><td>simply</td><td>1</td></tr>\n",
    "  <tr><td>restaurant</td><td>1</td></tr>\n",
    "  <tr><td>really</td><td>1</td></tr>\n",
    "  <tr><td>for</td><td>1</td></tr>\n",
    "  <tr><td>perfect</td><td>1</td></tr>\n",
    "  <tr><td>money</td><td>1</td></tr>\n",
    "  <tr><td>menu</td><td>1</td></tr>\n",
    "  <tr><td>absolutely</td><td>1</td></tr>\n",
    "  <tr><td>place</td><td>1</td></tr>\n",
    "  <tr><td>food</td><td>1</td></tr>\n",
    "  <tr><td>best</td><td>1</td></tr>\n",
    "  <tr><td>has</td><td>1</td></tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "\n",
    "<table>\n",
    "  <th>Negative BoW</th>\n",
    "  <tr><td><b>Words</b></td><td><b>Counts</b></td></tr>\n",
    "  <tr><td>food</td><td>3</td></tr>\n",
    "  <tr><td>most</td><td>2</td></tr>\n",
    "  <tr><td>disgusting</td><td>2</td></tr>\n",
    "  <tr><td>very</td><td>1</td></tr>\n",
    "  <tr><td>this</td><td>1</td></tr>\n",
    "  <tr><td>terrible</td><td>1</td></tr>\n",
    "  <tr><td>stay</td><td>1</td></tr>\n",
    "  <tr><td>serving</td><td>1</td></tr>\n",
    "  <tr><td>place</td><td>1</td></tr>\n",
    "  <tr><td>pathetic</td><td>1</td></tr>\n",
    "  <tr><td>i</td><td>1</td></tr>\n",
    "  <tr><td>have</td><td>1</td></tr>\n",
    "  <tr><td>has</td><td>1</td></tr>\n",
    "  <tr><td>experience</td><td>1</td></tr>\n",
    "  <tr><td>ever</td><td>1</td></tr>\n",
    "  <tr><td>away</td><td>1</td></tr>\n",
    "  <tr><td>had</td><td>1</td></tr>\n",
    "  <tr><td></td><td></td></tr>\n",
    "  <tr><td></td><td></td></tr>\n",
    "  <tr><td></td><td></td></tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XYX910rNOB0x"
   },
   "source": [
    "## The Testing Phase — Where Prediction Comes into the Play!\n",
    "\n",
    "Consider that now your model is given a restaurant review, “Very good food and service!!!”, and it needs to classify to what particular category it belongs to. A positive review or a negative one? We need to find the probability of this given review of belonging to each category and then we would assign it either a positive or a negative label depending upon for which particular category this test example was able to score more probability.\n",
    "\n",
    "## Finding Probability of a Given Test Example\n",
    "\n",
    "### Step # 1 : Preprocessing of Test Example\n",
    "\n",
    "Preprocess the test example in the same way as the training examples were preprocessed i.e changing examples to lower case and excluding everything apart from letters/alphabets.\n",
    "\n",
    "</br>\n",
    "\n",
    "<table>\n",
    "  <tr><td><b>Raw Test Example</b></td><td><b>Preprocessed Test Example</b></td></tr>\n",
    "  <tr><td>Very good food and service!!!</td><td>very good food and service</td></tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "</br>\n",
    "\n",
    "### Step # 2 : Tokenization of Preprocessed Test Example\n",
    "\n",
    "Tokenize the test example i.e split it into single words.\n",
    "\n",
    "</br>\n",
    "\n",
    "<table>\n",
    "  <tr><td>very</td><td>good</td><td>food</td><td>and</td><td>service</td></tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "</br>\n",
    "\n",
    "\n",
    "<b><i>A Quick Side Note</i></b> : You must be already familiar with the term “feature” in machine learning. Here, in Naive Bayes, each word in the vocabulary of each class of the training data set constitutes a categorical feature. This implies that counts of all the unique words (i.e vocabulary/vocab) of each class are basically a set of features for that particular class. And why do we need “counts” ? because we need a numeric representation of the categorical word features as the Naive Bayes Model/Algorithm requires numeric features to find out the probabilistic scores!\n",
    "\n",
    "</br>\n",
    "\n",
    "### Step # 3 : Using Probability to Predict Label for Tokenized Test Example\n",
    "\n",
    "</br>\n",
    "\n",
    "<!-- The not so intimidating mathematical form of finding probability -->\n",
    "\n",
    "The Probability of a Given <b>Test Example i</b> of belonging to class c:\n",
    "\n",
    "<b><i>p</i></b> (<b><i>i</i></b> belonging to class <b><i>c</i></b>) = product of two terms: product(<b>p</b> of a test word <b>j</b> in class <b>c</b>) and <b>p</b> of class <b>c</b>\n",
    "\n",
    "</br>\n",
    "\n",
    "*   let i = test example = “Very good food and service!!!”\n",
    "\n",
    "*   Total number of words in i = 5, so values of j (representing feature number) vary from 1 to 5.\n",
    "\n",
    "</br>\n",
    "\n",
    "Let’s map the above scenario to the given test example to make it more clear!\n",
    "\n",
    "</br>\n",
    "\n",
    "## Let’s start calculating values for these product terms.\n",
    "\n",
    "</br>\n",
    "\n",
    "### Step # 1 : Finding Value of the Term : p of class c\n",
    "\n",
    "Simply the Fraction of Each Category/Class in the Training Set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FmqBhoFuSE_5"
   },
   "outputs": [],
   "source": [
    "# !mv img/1_krSm7ixbHpcMDMqN4UgojA.png full_probability.png \n",
    "# ![](full_probability.png)\n",
    "# ![](img/probabil_pos_neg.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./full_probability.png\" width=\"60%\">\n",
    "\n",
    "p of class c for Positive & Negative categories:\n",
    "\n",
    "<img src=\"./img/probabil_pos_neg.png\" width=\"50%\">\n",
    "\n",
    "### Step # 2 : Finding value of term : product (p of a test word j in class c)\n",
    "\n",
    "Before we start deducing probability of a test word j in a specific class c let’s quickly get familiar with some easy peasy notation that is being used in the not so distant lines of this blog post:\n",
    "\n",
    "<img src=\"./img/1.png\" width=\"80%\">\n",
    "\n",
    "As we have only one example in our test set at the moment (for the sake of understanding), so i = 1.\n",
    "\n",
    "<img src=\"./img/2.png\" width=\"60%\">\n",
    "\n",
    "<b><i>A Quick Side Note:</i></b> During test time/prediction time, we map every word of test example against it’s count that was found during training phase. So, in this case, we are looking for in total 5 word counts for this given test example.\n",
    "\n",
    "### Finding Probability of a Test Word “ j ” in class c\n",
    "\n",
    "Before we start calculating product ( p of a test word “ j ” in class c ), we obviously first need to determine p of a test word “ j ” in class c . There are two ways of doing this as specified below — which one should be actually followed and rather is practically used will be discovered in just a few minutes…\n",
    "\n",
    "<img src=\"./img/3.png\" width=\"60%\">\n",
    "\n",
    "### let’s first try finding probabilities using method number 1 :\n",
    "\n",
    "<img src=\"./img/4.png\" width=\"60%\">\n",
    "\n",
    "Now we can multiply the probabilities of individual words ( as found above ) in order to find the numerical value of the term : \n",
    "<b>product</b> ( <b>p</b> of a test word <b>“ j ”</b> in class <b>c</b> )\n",
    "\n",
    "\n",
    "<b>The Common Pitfall of Zero Probabilities!</b>\n",
    "\n",
    "<img src=\"./img/5.png\" width=\"60%\">\n",
    "\n",
    "By now, we have numerical values for both the terms i.e ( <b>p</b> of class <b>c</b> and <b>product</b> ( <b>p</b> of a test word <b>“ j ”</b> in class <b>c</b> ) ) for both the classes . So we can multiply both of these terms in order to determine <b>p</b> ( <b>i</b> belonging to class <b>c</b> ) for both the categories. This is demonstrated below :\n",
    "\n",
    "<img src=\"./img/6.png\" width=\"60%\">\n",
    "\n",
    "The <b>p</b> ( <b>i</b> belonging to class <b>c</b> ) turns out to be <b>zero for both the categories!!!</b> but clearly the test example “Very good food and service!!!” belongs to positive class! Clearly, this happened because the <b>product</b> ( <b>p</b> of a test word \n",
    "<b>“ j ”</b> in class <b>c</b> ) <b>was zero for both the categories</b> and this in turn was zero because <b>a few words in the given test example (highlighted in orange) NEVER EVER appeared in our training dataset and hence their probability was zero! and clearly they have caused all the destruction!</b>\n",
    "\n",
    "So does this imply that whenever a word that appears in the test example but never ever occurred in the training dataset will always cause such destruction ? and in such case our trained model will never be able to predict the correct sentiment? It will just randomly pick positive or negative category since both have same zero probability and predict wrongly? The answer is NO! This is where the second method (numbered 2) comes into play and infact this is the mathematical formula that is actually used to deduce <b>p</b> ( <b>i</b> belonging to class <b>c</b> ) . But before we move on the method number 2, we should first get familiar with it’s mathematical brainy stuff!\n",
    "\n",
    "<img src=\"./img/7.png\" width=\"60%\">\n",
    "\n",
    "So now <b>after adding pseudocounts of 1’s , the probability p of a test word that NEVER EVER APPEARED IN THE TRAINING DATASET WILL NEVER BE ZERO</b> and therefore, the numerical value of the term <b>product</b> ( <b>p</b> of a test word <b>“ j ”</b> in class <b>c</b> ) will never end up as zero which in turn implies that \n",
    "<b>p</b> ( <b>i</b> belonging to class <b>c</b> ) will never be zero as well! So all is well and no more destruction by zero probabilities!\n",
    "\n",
    "<b>So the numerator term of method number 2 will have an added 1 as we have added a one for every word in the vocabulary and so it becomes:</b>\n",
    "\n",
    "<img src=\"./img/8.png\" width=\"60%\">\n",
    "\n",
    "Similarly the denominator becomes :\n",
    "\n",
    "<img src=\"./img/9.png\" width=\"60%\">\n",
    "\n",
    "And so the complete formula :\n",
    "\n",
    "<img src=\"./img/10.png\" width=\"60%\">\n",
    "\n",
    "<img src=\"./img/11.png\" width=\"60%\">\n",
    "\n",
    "Probabilities of Positive & Negative Class:\n",
    "\n",
    "<img src=\"./img/12.png\" width=\"60%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now finding probabilities using method number 2 :\n",
    "\n",
    "Handling of Zero Probabilities : These act like failsafe probabilities !\n",
    "\n",
    "<img src=\"./img/13.png\" width=\"60%\">\n",
    "\n",
    "<img src=\"./img/14.png\" width=\"60%\">\n",
    "\n",
    "<img src=\"./img/15.png\" width=\"60%\">\n",
    "\n",
    "Now as probability of the test example, ”Very good food and service!!!” is more for the positive class i.e 9.33E-09 as compared to the negative class (i.e 7.74E-09), so we can predict it as a Positive Sentiment ! And that is how we simply predict a label for a test/unseen example\n",
    "\n",
    "<b><i>A Quick Side Note :</i></b> As like every other machine learning algorithm, Naive Bayes too needs a validation set to assess the trained model’s effectiveness. But, since this post was aimed to focus on the algorithmic insights, so I deliberately avoided it and directly jumped to the testing part\n",
    "\n",
    "## Digging Deeper into the Mathematics of Probability\n",
    "\n",
    "Now that you have built a basic understanding of the probabilistic calculations needed to train the Naive Bayes Model and then using it to predict probability for the given test sentence, I will now dig deeper into the probabilistic details. \n",
    "While doing the calculations of probability of the given test sentence in the above section, we did nothing but implemented the given probabilistic formula for our prediction at test time:\n",
    "\n",
    "<img src=\"./img/16.png\" width=\"60%\">\n",
    "\n",
    "### Decoding the above mathematical equation :\n",
    "\n",
    "“|” = refers to a state which has already been given / or some filtering criteria\n",
    "\n",
    "“c” = class/category\n",
    "\n",
    "“x” = test example/test sentence\n",
    "\n",
    "<b>p (c|x) = given test example x</b>, what is it’s <b>probability of belonging to class c</b>. This is also known as posterior probability. This is <b>conditional probability that is to be found for the given test example x for each of the given training classes</b>.\n",
    "\n",
    "<b>p(x|c)=given class c</b>, what is the <b>probability of example x belonging to class c</b>. This is also known as likelihood as it implies how much likely does example <b>x</b> belongs to class <b>c</b>. This is <b>conditional probability</b> too as we are finding probability of <b>x</b> out of total instances of class <b>c</b> only i.e we have <b>restricted/conditioned our search space to class c while finding the probability of x. We calculate this probability using the counts of words that are determined during the training phase.</b>\n",
    "\n",
    "Here “ j ” represents a class and k represents a feature\n",
    "\n",
    "<img src=\"./img/17.png\" width=\"60%\">\n",
    "\n",
    "We implicitly used this formula twice above in the calculations sections as we had two classes. Remember finding the numerical value of <b>product</b> ( <b>p</b> of a test word <b>“ j ”</b> in class <b>c</b> ) ?\n",
    "\n",
    "<img src=\"./img/18.png\" width=\"60%\">\n",
    "\n",
    "<b>p</b> = This implies the <b>probability of class c</b>. This is also known as prior probability/unconditional probability. This is unconditional probability. We calculated this too earlier above in the probability calculations sections ( in Step # 1 which was finding value of term : <b>p</b> of class <b>c</b> )\n",
    "\n",
    "<b>p(x)</b> = This is also known as <b>normalizing constant so that the probability p(c|x) does actually falls in the range [0,1]</b>. So if you remove this, the probability <b>p(c|x)</b> may not necessarily fall in the range of [0,1]. Intuitively this means probability of example <b>x</b> under any circumstances or irrespective of it’s class labels i.e whether positive or negative.\n",
    "This is also reflected in total probability theorem which is used to <b>calculate p(x)</b> and dictates that to find <b>p(x)</b>, we will find it’s probability in all given classes (because it is unconditional probability)and simply add them :\n",
    "\n",
    "Total Probability Theorem\n",
    "<img src=\"./img/19.png\" width=\"60%\">\n",
    "\n",
    "This implies that if we have two classes then we would have two terms, so in our particular case of positive and negative sentiments:\n",
    "\n",
    "Total Probability Theorem for Two Classes\n",
    "<img src=\"./img/20.png\" width=\"60%\">\n",
    "\n",
    "<b>Did we use it in the above calculations? No we did not.</b> Why??? because we are comparing probabilities of positive and negative class and since the denominator remains the same, so in this particular case, omitting out the same denominator doesn’t affect the prediction by our trained model. It simply cancels out for both classes. So although we can include it but there is no such logical reason to do so. <b>But again as we have eliminated the normalization constant, the probability p(c|x) may not necessarily fall in the range of [0,1]</b>\n",
    "\n",
    "</br>\n",
    "\n",
    "## Avoiding the Common Pitfall of The Underflow Error!\n",
    "\n",
    "* If you noticed, the numerical values of probabilities of words ( i.e p of a test word “ j ” in class c ) were quite small. And therefore, multiplying all these tiny probabilities to find product ( p of a test word “ j ” in class c ) will yield even a more smaller numerical value that often results in underflow which obviously means that for that given test sentence, the trained model will fail to predict it’s category/sentiment. So to avoid this underflow error, we take help of mathematical log as follows :\n",
    "\n",
    "<img src=\"./img/21.png\" width=\"60%\">\n",
    "\n",
    "* So now instead of multiplication of the tiny individual word probabilities, we will simply add them. And why only log? why not any other function? Because log increases or decreases monotonically which means that it will not affect the order of probabilities. Probabilities that were smaller will still stay smaller after the log has been applied on them and vice versa. so let’s say that a test word “is” has a smaller probability than the test word “happy”, so after passing these through log would although increase their magnitude but “is” would still have a smaller probability than “happy”. Therefore, without affecting the predictions of our trained model, we can effectively avoid the common pitfall of underflow error.\n",
    "\n",
    "</br>\n",
    "\n",
    "# Concluding Notes….\n",
    "\n",
    "* Although we live in a age of API’s and practically rarely code from scratch. But understanding the algorithmic theory in depth is extremely vital to develop a sound understanding of how the machine learning algorithms actually work. It is only the key understanding which actually differentiates a true data scientist from a naive one and what actually matters when training a really good model. So before moving to API’s , I personally believe that a true data scientist should code from scratch to actually see behind the numbers and the reason why a particular algorithm is better than the other.\n",
    "\n",
    "\n",
    "* One of the best characteristics of the Naive Bayes Model is that you can improve it’s accuracy by simply updating it with new vocabulary words instead of always retraining it. You will just need to add words to the vocabulary and update the words counts accordingly. That’s it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy pandas scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KFHMeRzZXdE0"
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from collections import defaultdict\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dgtfV1iFXgTS"
   },
   "outputs": [],
   "source": [
    "def preprocess_string(str_arg):\n",
    "    \n",
    "    \"\"\"\"\n",
    "        Parameters:\n",
    "        ----------\n",
    "        str_arg: example string to be preprocessed\n",
    "        \n",
    "        What the function does?\n",
    "        -----------------------\n",
    "        Preprocess the string argument - str_arg - such that :\n",
    "        1. everything apart from letters is excluded\n",
    "        2. multiple spaces are replaced by single space\n",
    "        3. str_arg is converted to lower case \n",
    "        \n",
    "        Example:\n",
    "        --------\n",
    "        Input :  Menu is absolutely perfect,loved it!\n",
    "        Output:  ['menu', 'is', 'absolutely', 'perfect', 'loved', 'it']\n",
    "        \n",
    "        Returns:\n",
    "        ---------\n",
    "        Preprocessed string \n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    cleaned_str=re.sub('[^a-z\\s]+',' ',str_arg,flags=re.IGNORECASE) #every char except alphabets is replaced\n",
    "    cleaned_str=re.sub('(\\s+)',' ',cleaned_str) #multiple spaces are replaced by single space\n",
    "    cleaned_str=cleaned_str.lower() #converting the cleaned string to lower case\n",
    "    \n",
    "    return cleaned_str # returning the preprocessed string "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VVkLTYv_Xtjw"
   },
   "outputs": [],
   "source": [
    "class NaiveBayes:\n",
    "    \n",
    "    def __init__(self,unique_classes):\n",
    "        \n",
    "        self.classes=unique_classes # Constructor is sinply passed with unique number of classes of the training set\n",
    "        \n",
    "\n",
    "    def addToBow(self,example,dict_index):\n",
    "        \n",
    "        '''\n",
    "            Parameters:\n",
    "            1. example \n",
    "            2. dict_index - implies to which BoW category this example belongs to\n",
    "            What the function does?\n",
    "            -----------------------\n",
    "            It simply splits the example on the basis of space as a tokenizer and adds every tokenized word to\n",
    "            its corresponding dictionary/BoW\n",
    "            Returns:\n",
    "            ---------\n",
    "            Nothing\n",
    "        \n",
    "       '''\n",
    "        \n",
    "        if isinstance(example,np.ndarray): example=example[0]\n",
    "     \n",
    "        for token_word in example.split(): #for every word in preprocessed example\n",
    "          \n",
    "            self.bow_dicts[dict_index][token_word]+=1 #increment in its count\n",
    "            \n",
    "    def train(self,dataset,labels):\n",
    "        \n",
    "        '''\n",
    "            Parameters:\n",
    "            1. dataset - shape = (m X d)\n",
    "            2. labels - shape = (m,)\n",
    "            What the function does?\n",
    "            -----------------------\n",
    "            This is the training function which will train the Naive Bayes Model i.e compute a BoW for each\n",
    "            category/class. \n",
    "            Returns:\n",
    "            ---------\n",
    "            Nothing\n",
    "        \n",
    "        '''\n",
    "    \n",
    "        self.examples=dataset\n",
    "        self.labels=labels\n",
    "        self.bow_dicts=np.array([defaultdict(lambda:0) for index in range(self.classes.shape[0])])\n",
    "        \n",
    "        #only convert to numpy arrays if initially not passed as numpy arrays - else its a useless recomputation\n",
    "        \n",
    "        if not isinstance(self.examples,np.ndarray): self.examples=np.array(self.examples)\n",
    "        if not isinstance(self.labels,np.ndarray): self.labels=np.array(self.labels)\n",
    "            \n",
    "        #constructing BoW for each category\n",
    "        for cat_index,cat in enumerate(self.classes):\n",
    "          \n",
    "            all_cat_examples=self.examples[self.labels==cat] #filter all examples of category == cat\n",
    "            \n",
    "            #get examples preprocessed\n",
    "            \n",
    "            cleaned_examples=[preprocess_string(cat_example) for cat_example in all_cat_examples]\n",
    "            \n",
    "            cleaned_examples=pd.DataFrame(data=cleaned_examples)\n",
    "            \n",
    "            #now costruct BoW of this particular category\n",
    "            np.apply_along_axis(self.addToBow,1,cleaned_examples,cat_index)\n",
    "            \n",
    "                \n",
    "        ###################################################################################################\n",
    "        \n",
    "        '''\n",
    "            Although we are done with the training of Naive Bayes Model BUT!!!!!!\n",
    "            ------------------------------------------------------------------------------------\n",
    "            Remember The Test Time Forumla ? : {for each word w [ count(w|c)+1 ] / [ count(c) + |V| + 1 ] } * p(c)\n",
    "            ------------------------------------------------------------------------------------\n",
    "            \n",
    "            We are done with constructing of BoW for each category. But we need to precompute a few \n",
    "            other calculations at training time too:\n",
    "            1. prior probability of each class - p(c)\n",
    "            2. vocabulary |V| \n",
    "            3. denominator value of each class - [ count(c) + |V| + 1 ] \n",
    "            \n",
    "            Reason for doing this precomputing calculations stuff ???\n",
    "            ---------------------\n",
    "            We can do all these 3 calculations at test time too BUT doing so means to re-compute these \n",
    "            again and again every time the test function will be called - this would significantly\n",
    "            increase the computation time especially when we have a lot of test examples to classify!!!).  \n",
    "            And moreover, it doensot make sense to repeatedly compute the same thing - \n",
    "            why do extra computations ???\n",
    "            So we will precompute all of them & use them during test time to speed up predictions.\n",
    "            \n",
    "        '''\n",
    "        \n",
    "        ###################################################################################################\n",
    "      \n",
    "        prob_classes=np.empty(self.classes.shape[0])\n",
    "        all_words=[]\n",
    "        cat_word_counts=np.empty(self.classes.shape[0])\n",
    "        for cat_index,cat in enumerate(self.classes):\n",
    "           \n",
    "            #Calculating prior probability p(c) for each class\n",
    "            prob_classes[cat_index]=np.sum(self.labels==cat)/float(self.labels.shape[0]) \n",
    "            \n",
    "            #Calculating total counts of all the words of each class \n",
    "            count=list(self.bow_dicts[cat_index].values())\n",
    "            cat_word_counts[cat_index]=np.sum(np.array(list(self.bow_dicts[cat_index].values())))+1 # |v| is remaining to be added\n",
    "            \n",
    "            #get all words of this category                                \n",
    "            all_words+=self.bow_dicts[cat_index].keys()\n",
    "                                                     \n",
    "        \n",
    "        #combine all words of every category & make them unique to get vocabulary -V- of entire training set\n",
    "        \n",
    "        self.vocab=np.unique(np.array(all_words))\n",
    "        self.vocab_length=self.vocab.shape[0]\n",
    "                                  \n",
    "        #computing denominator value                                      \n",
    "        denoms=np.array([cat_word_counts[cat_index]+self.vocab_length+1 for cat_index,cat in enumerate(self.classes)])                                                                          \n",
    "      \n",
    "        '''\n",
    "            Now that we have everything precomputed as well, its better to organize everything in a tuple \n",
    "            rather than to have a separate list for every thing.\n",
    "            \n",
    "            Every element of self.cats_info has a tuple of values\n",
    "            Each tuple has a dict at index 0, prior probability at index 1, denominator value at index 2\n",
    "        '''\n",
    "        \n",
    "        self.cats_info=[(self.bow_dicts[cat_index],prob_classes[cat_index],denoms[cat_index]) for cat_index,cat in enumerate(self.classes)]                               \n",
    "        self.cats_info=np.array(self.cats_info)                                 \n",
    "                                              \n",
    "                                              \n",
    "    def getExampleProb(self,test_example):                                \n",
    "        \n",
    "        '''\n",
    "            Parameters:\n",
    "            -----------\n",
    "            1. a single test example \n",
    "            What the function does?\n",
    "            -----------------------\n",
    "            Function that estimates posterior probability of the given test example\n",
    "            Returns:\n",
    "            ---------\n",
    "            probability of test example in ALL CLASSES\n",
    "        '''                                      \n",
    "                                              \n",
    "        likelihood_prob=np.zeros(self.classes.shape[0]) #to store probability w.r.t each class\n",
    "        \n",
    "        #finding probability w.r.t each class of the given test example\n",
    "        for cat_index,cat in enumerate(self.classes): \n",
    "                             \n",
    "            for test_token in test_example.split(): #split the test example and get p of each test word\n",
    "                \n",
    "                ####################################################################################\n",
    "                                              \n",
    "                #This loop computes : for each word w [ count(w|c)+1 ] / [ count(c) + |V| + 1 ]                               \n",
    "                                              \n",
    "                ####################################################################################                              \n",
    "                \n",
    "                #get total count of this test token from it's respective training dict to get numerator value                           \n",
    "                test_token_counts=self.cats_info[cat_index][0].get(test_token,0)+1\n",
    "                \n",
    "                #now get likelihood of this test_token word                              \n",
    "                test_token_prob=test_token_counts/float(self.cats_info[cat_index][2])                              \n",
    "                \n",
    "                #remember why taking log? To prevent underflow!\n",
    "                likelihood_prob[cat_index]+=np.log(test_token_prob)\n",
    "                                              \n",
    "        # we have likelihood estimate of the given example against every class but we need posterior probility\n",
    "        post_prob=np.empty(self.classes.shape[0])\n",
    "        for cat_index,cat in enumerate(self.classes):\n",
    "            post_prob[cat_index]=likelihood_prob[cat_index]+np.log(self.cats_info[cat_index][1])                                  \n",
    "      \n",
    "        return post_prob\n",
    "    \n",
    "   \n",
    "    def test(self,test_set):\n",
    "      \n",
    "        '''\n",
    "            Parameters:\n",
    "            -----------\n",
    "            1. A complete test set of shape (m,)\n",
    "            \n",
    "            What the function does?\n",
    "            -----------------------\n",
    "            Determines probability of each test example against all classes and predicts the label\n",
    "            against which the class probability is maximum\n",
    "            Returns:\n",
    "            ---------\n",
    "            Predictions of test examples - A single prediction against every test example\n",
    "        '''       \n",
    "       \n",
    "        predictions=[] #to store prediction of each test example\n",
    "        for example in test_set: \n",
    "                                              \n",
    "            #preprocess the test example the same way we did for training set exampels                                  \n",
    "            cleaned_example=preprocess_string(example) \n",
    "             \n",
    "            #simply get the posterior probability of every example                                  \n",
    "            post_prob=self.getExampleProb(cleaned_example) #get prob of this example for both classes\n",
    "            \n",
    "            #simply pick the max value and map against self.classes!\n",
    "            predictions.append(self.classes[np.argmax(post_prob)])\n",
    "                \n",
    "        return np.array(predictions) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PJpgmXpRX1dQ"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\"\"\" \n",
    "fetch_20newsgroups is a dataset that has 20 categories but we will restrict\n",
    "the categories to 4 for the time being \n",
    "\"\"\"\n",
    "categories=['alt.atheism', 'soc.religion.christian','comp.graphics', 'sci.med'] \n",
    "newsgroups_train=fetch_20newsgroups(subset='train',categories=categories)\n",
    "\n",
    "\"\"\"\n",
    "    Some training data is being loaded where training examples are saved\n",
    "    in train_data and train labels are saved in train_labels\n",
    "\"\"\"\n",
    "\n",
    "train_data=newsgroups_train.data #getting all trainign examples\n",
    "train_labels=newsgroups_train.target #getting training labels\n",
    "#print (\"Total Number of Training Examples: \",len(train_data)) # Outputs -> Total Number of Training Examples:  2257\n",
    "#print (\"Total Number of Training Labels: \",len(train_labels)) # Outputs -> #Total Number of Training Labels:  2257\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "W7AEvgP5X853",
    "outputId": "4be98657-2d2b-4679-8a9b-cf178128b55e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------- Training In Progress --------------------\n",
      "----------------- Training Completed ---------------------\n"
     ]
    }
   ],
   "source": [
    "nb=NaiveBayes(np.unique(train_labels)) #instantiate a NB class object\n",
    "print (\"---------------- Training In Progress --------------------\")\n",
    " \n",
    "nb.train(train_data,train_labels) #start tarining by calling the train function\n",
    "print ('----------------- Training Completed ---------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "MbzfRY3RYKba",
    "outputId": "9b2ba032-4226-4fac-b242-f1017810b8ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Test Examples:  1502\n",
      "Number of Test Labels:  1502\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Some test data is being loaded where test examples are saved in test_data\n",
    "    and test labels are saved in test_labels\n",
    "\"\"\"\n",
    "newsgroups_test=fetch_20newsgroups(subset='test',categories=categories) #loading test data\n",
    "test_data=newsgroups_test.data #get test set examples\n",
    "test_labels=newsgroups_test.target #get test set labels\n",
    "\n",
    "print (\"Number of Test Examples: \",len(test_data)) # Output : Number of Test Examples:  1502\n",
    "print (\"Number of Test Labels: \",len(test_labels)) # Output : Number of Test Labels:  1502"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "4Xy3GX5nYRGe",
    "outputId": "c0a88cf6-9e31-41a5-e603-2efb1ce4c1e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Examples:  1502\n",
      "Test Set Accuracy:  93.87483355525966 %\n"
     ]
    }
   ],
   "source": [
    "pclasses=nb.test(test_data) #get predcitions for test set\n",
    "\n",
    "#check how many predcitions actually match original test labels\n",
    "test_acc=np.sum(pclasses==test_labels)/float(test_labels.shape[0]) \n",
    "\n",
    "print (\"Test Set Examples: \",test_labels.shape[0]) # Outputs : Test Set Examples:  1502\n",
    "print (\"Test Set Accuracy: \",test_acc*100,\"%\") # Outputs : Test Set Accuracy:  93.8748335553 %\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Naive Bayes from scratch.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
